
@report{arfireGraphicalKnowledgeRepresentation2006,
  type = {Stage de fin d'études},
  title = {Graphical {{Knowledge Representation}}},
  author = {Arfire, Doru},
  date = {2006-06},
  institution = {{Universitatea Politehnica Bucuresti}},
  langid = {english},
  keywords = {\#nosource,Master,VAE}
}

@inproceedings{bourdaTuteursIntelligentsBoucler2018,
  title = {Tuteurs intelligents : boucler la boucle},
  booktitle = {PFIA 2018 - Journée « IA pour l’éducation »},
  author = {Bourda, Yolaine and Chaudet, Claude and Choffin, Benoît and Parmentier, Jeanne and Popineau, Fabrice and Vie, Jill-Jênn},
  date = {2018-07},
  pages = {4},
  url = {https://hal.archives-ouvertes.fr/hal-02197685},
  langid = {french},
  keywords = {⛔ No DOI found,EIAH,poster,VAE},
  file = {files/9031/Bourda et al. - 2018 - Tuteurs intelligents  boucler la boucle.pdf}
}

@article{chaixXEmTeXIntegratedPlatform2003,
  title = {{{XEmTeX}}: {{An}} Integrated Platform for High Quality Scientific Typesetting},
  author = {Chaix, Marie-Louise and Popineau, Fabrice},
  date = {2003-07},
  keywords = {\#nosource,⛔ No DOI found,poster,TeX}
}

@article{chaixXEmTeXProject2003,
  title = {The {{XEmTeX}} Project},
  author = {Chaix, Marie-Louise and Popineau, Fabrice},
  date = {2003-06},
  journaltitle = {TUGBoat},
  volume = {24},
  number = {3},
  pages = {415--419},
  keywords = {⛔ No DOI found,conf,TeX},
  file = {files/9149/Chaix et Popineau - 2003 - The XEmTeX project.pdf}
}

@thesis{choffinAlgorithmesEspacementAdaptatif2021,
  type = {Thèse de doctorat},
  title = {Algorithmes d’espacement adaptatif de l’apprentissage pour l’optimisation de la maîtrise à long terme de composantes de connaissance},
  author = {Choffin, Benoît},
  date = {2021-01-28},
  institution = {{université Paris-Saclay}},
  url = {https://theses.fr/2021UPASG001},
  urldate = {2021-08-07},
  abstract = {Entre acquérir de nouvelles connaissances et revoir les anciennes pour en atténuer l’oubli, les apprenants peuvent avoir du mal à organiser efficacement leur temps d’apprentissage. Les algorithmes d’espacement adaptatif de l’apprentissage, tels SuperMemo, permettent d’aider les apprenants à résoudre cet arbitrage. Ces algorithmes planifient les révisions successives d’une même connaissance de manière optimale et personnalisée en tenant compte des besoins de chaque apprenant. Comparé à un espacement temporel entre les révisions identique pour tous les individus, plusieurs expériences montrent que l’espacement adaptatif maintient un plus haut degré d’ancrage en mémoire à long terme des informations apprises.Jusqu’ici, la recherche sur l’espacement adaptatif de l’apprentissage s’est concentrée sur la mémorisation pure de connaissances simples, représentées souvent par le biais de flashcards. Or, plusieurs études en psychologie cognitive montrent que les bénéfices de l’espacement de l’apprentissage sur la mémorisation à long terme s’étendent aussi à des connaissances plus complexes, telles que l’apprentissage de concepts et de procédures en mathématiques. Dans cette thèse, nous avons donc cherché à développer des algorithmes d’espacement adaptatif et personnalisé de l’apprentissage de composantes de connaissance (CC).Dans un premier temps, nous proposons un nouveau modèle statistique de l’apprentissage et l’oubli de CC, appelé DAS3H, et montrons empiriquement qu’il possède de meilleures performances prédictives que plusieurs modèles de l’apprenant en fouille de données éducatives. Ensuite, nous développons plusieurs heuristiques d’espacement adaptatif pour la maîtrise à long terme de CC et comparons leurs performances sur des données simulées. Deux de ces heuristiques reposent sur le modèle DAS3H pour sélectionner la CC à faire réviser à un instant donné. Nous proposons en outre une nouvelle procédure gloutonne pour sélectionner le sous-ensemble de CC le plus prometteur au lieu de la meilleure CC à faire réviser. Enfin, dans le dernier chapitre de cette thèse, nous développons AC4S, un algorithme d’apprentissage par renforcement profond pour l’espacement adaptatif de l’apprentissage de CC. Nous comparons cette approche fondée sur les données à nos méthodes heuristiques, présentées précédemment.},
  editora = {Bourda, Yolaine and Popineau, Fabrice},
  editoratype = {collaborator},
  langid = {french},
  keywords = {Adaptive spacing,Apprentissage par renforcement profond,Apprentissage profond,Calcul adaptatif,Composantes de connaissance (CC),Deep reinforcement learning,Educational data mining,Espacement adaptatif de l’apprentissage,Exploration de données,Fouille de données éducatives,Knowledge components (KCs),Planification de révisions,Review scheduling,Thesis},
  file = {files/9177/Choffin - 2021 - Algorithmes d’espacement adaptatif de l’apprentiss.pdf}
}

@inproceedings{choffinDAS3HModelingStudent2019a,
  title = {{{DAS3H}}: {{Modeling Student Learning}} and {{Forgetting}} for {{Optimally Scheduling Distributed Practice}} of {{Skills}}},
  booktitle = {Proceedings of the 12th International Conference on Educational Data Mining, {{EDM}} 2019, Montréal, Canada, July 2-5, 2019},
  author = {Choffin, Benoît and Popineau, Fabrice and Bourda, Yolaine and Vie, Jill-Jênn},
  editor = {Desmarais, Michel C. and Lynch, Collin F. and Merceron, Agathe and Nkambou, Roger},
  date = {2019},
  publisher = {{International Educational Data Mining Society (IEDMS)}},
  location = {{Montréal, Canada}},
  url = {http://arxiv.org/abs/1905.06873},
  abstract = {Spaced repetition is among the most studied learning strategies in the cognitive science literature. It consists in temporally distributing exposure to an information so as to improve long-term memorization. Providing students with an adaptive and personalized distributed practice schedule would benefit more than just a generic scheduler. However, the applicability of such adaptive schedulers seems to be limited to pure memorization, e.g. flashcards or foreign language learning. In this article, we first frame the research problem of optimizing an adaptive and personalized spaced repetition scheduler when memorization concerns the application of underlying multiple skills. To this end, we choose to rely on a student model for inferring knowledge state and memory dynamics on any skill or combination of skills. We argue that no knowledge tracing model takes both memory decay and multiple skill tagging into account for predicting student performance. As a consequence, we propose a new student learning and forgetting model suited to our research problem: DAS3H builds on the additive factor models and includes a representation of the temporal distribution of past practice on the skills involved by an item. In particular, DAS3H allows the learning and forgetting curves to differ from one skill to another. Finally, we provide empirical evidence on three real-world educational datasets that DAS3H outperforms other state-of-the-art EDM models. These results suggest that incorporating both item-skill relationships and forgetting effect improves over student models that consider one or the other.},
  eventtitle = {Educational {{Data Mining}} 2019},
  langid = {english},
  keywords = {\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computers and Society,conf,EIAH,Statistics - Applications,Statistics - Machine Learning},
  file = {files/9029/Choffin et al. - 2019 - DAS3H Modeling Student Learning and Forgetting fo.pdf}
}

@inproceedings{choffinEvaluatingDAS3HEdNet2021,
  title = {Evaluating {{DAS3H}} on the {{EdNet Dataset}}},
  booktitle = {{{AAAI}} 2021 - {{The}} 35th {{Conference}} on {{Artificial Intelligence}} / {{Imagining Post-COVID Education}} with {{AI}}},
  author = {Choffin, Benoît and Popineau, Fabrice and Bourda, Yolaine and Vie, Jill-Jênn},
  date = {2021},
  pages = {8},
  url = {https://hal.archives-ouvertes.fr/hal-03175874/document},
  abstract = {The EdNet dataset is a massive English language dataset that poses unique challenges for student performance prediction. In this paper, we describe and comment the results of our award-winning model DAS3H in the context of knowledge tracing in EdNet.},
  eventtitle = {{{AAAI}} 2021 - {{The}} 35th {{Conference}} on {{Artificial Intelligence}} / {{Imagining Post-COVID Education}} with {{AI}}},
  langid = {english},
  keywords = {⛔ No DOI found,conf,EIAH},
  file = {files/9021/Choffin et al. - 2021 - Evaluating DAS3H on the EdNet Dataset.pdf}
}

@article{choffinExtendingAdaptiveSpacing2021,
  title = {Extending {{Adaptive Spacing Heuristics}} to {{Multi-Skill Items}}},
  author = {Choffin, Benoît and Popineau, Fabrice and Bourda, Yolaine},
  date = {2021-10-31},
  journaltitle = {Journal of Educational Data Mining},
  shortjournal = {JEDM},
  volume = {13},
  number = {3},
  pages = {69--102},
  issn = {2157-2100},
  doi = {10/gm9xv3},
  url = {https://jedm.educationaldatamining.org},
  urldate = {2021-11-01},
  abstract = {Adaptive spacing algorithms are powerful tools for helping learners manage their study time efficiently. By personalizing the temporal distribution of retrieval practice of a given piece of knowledge, they improve learners' long-term memory retention compared to fixed review schedules. However, such algorithms are generally designed for the pure memorization of single items, such as vocabulary words. Yet, the spacing effect has been shown to extend to more complex knowledge, such as the practice of mathematical skills. In this article, we extend three adaptive spacing heuristics from the literature for selecting the best skill to review at any timestamp given a student's past study history. In real-world educational settings, items generally involve multiple skills at the same time. Thus, we also propose a multi-skill version for two of these heuristics: instead of selecting one single skill, they select with a greedy procedure the most promising subset of skills to review. To compare these five heuristics, we develop a synthetic experimental framework that simulates student learning and forgetting trajectories with a student model. We run multiple synthetic experiments on large cohorts of 500 simulated students and publicly release the code for these experiments. Our results highlight the strengths and weaknesses of each heuristic in terms of performance, robustness, and complexity. Finally, we find evidence that selecting the best subset of skills yields better retention compared to selecting the single best skill to review.},
  langid = {english},
  keywords = {⛔ No DOI found,EIAH,journal},
  file = {files/9765/Choffin et al. - 2021 - Extending Adaptive Spacing Heuristics to Multi-Ski.pdf}
}

@article{choffinModellingStudentLearning2020,
  title = {Modelling Student Learning and Forgetting for Optimally Scheduling Skill Review},
  author = {Choffin, Benoît and Popineau, Fabrice and Bourda, Yolaine},
  date = {2020},
  journaltitle = {ERCIM News},
  volume = {2020},
  number = {120},
  url = {https://ercim-news.ercim.eu/en120/special/modelling-student-learning-and-forgetting-for-optimally-scheduling-skill-review},
  keywords = {⛔ No DOI found,artificial intelligence in education,EIAH,learning analytics,other,otherpub},
  file = {files/9028/Choffin et al. - 2020 - Modelling student learning and forgetting for opti.pdf}
}

@report{dincaConstructionSiteWeb2000,
  type = {Stage de fin d'études},
  title = {Construction d'un Site Web Adaptatif},
  author = {Dinca, Claudiu},
  date = {2000-09},
  institution = {{Universitatea Politehnica Bucuresti}},
  langid = {french},
  keywords = {\#nosource,Master,VAE}
}

@inproceedings{dubusFormalApproachPersonalization2011,
  title = {A {{Formal Approach}} to {{Personalization}}},
  booktitle = {2011 {{IEEE}} 23rd {{International Conference}} on {{Tools}} with {{Artificial Intelligence}}},
  author = {Dubus, Georges and Popineau, Fabrice and Bourda, Yolaine},
  date = {2011-11},
  pages = {233--238},
  publisher = {{IEEE Computer Society}},
  location = {{Boca Raton, FL, USA}},
  doi = {10/fzhff2},
  abstract = {Personalized systems are a response to the increasing number of resources on the Internet. In order to facilitate their design and creation, we aim at formalizing them. In this paper, we consider the relationship between a personalized application and its non-personalized counterpart. We argue that a personalized application is a formal extension of a non-personalized one. We aim at characterizing the syntactic differences between the expression of the personalized and non-personalized versions of the application. Situation calculus is our framework to formalize applications. We introduce two scenarios of non-personalized application that we personalize to illustrate our approach.},
  eventtitle = {2011 {{IEEE}} 23rd {{International Conference}} on {{Tools}} with {{Artificial Intelligence}}},
  keywords = {\#nosource,⛔ No DOI found,Adaptation models,adaptive systems,Adaptive systems,Calculus,conf,EIAH,Games,personalization,Robot sensing systems,situation calculus,VAE,weaving,Weaving,web application},
  file = {files/9104/Dubus et al. - 2011 - A Formal Approach to Personalization.pdf}
}

@inproceedings{dubusParametricReasoningAgents2013,
  title = {Parametric {{Reasoning Agents Extend}} the {{Control}} over {{Standard Behaviors}}},
  booktitle = {2013 {{IEEE}}/{{WIC}}/{{ACM International Joint Conferences}} on {{Web Intelligence}} ({{WI}}) and {{Intelligent Agent Technologies}} ({{IAT}})},
  author = {Dubus, Georges and Popineau, Fabrice and Bourda, Yolaine and Sansonnet, Jean-Paul},
  date = {2013-11},
  volume = {2},
  pages = {163--170},
  publisher = {{IEEE}},
  location = {{Atlanta, GA, USA}},
  doi = {10.1109/WI-IAT.2013.105},
  url = {http://ieeexplore.ieee.org/document/6690785/},
  urldate = {2019-12-21},
  abstract = {In this paper, we study how to extend the control over the behavioral space of logically specified agents so as they can take into account parameters issuing from constraints and/or preferences that are external to their core reasoning process. Our approach is supported by the proposition of a set of generic transformations to apply to the original agent program that is expressed in IndiGolog, a variant of the Golog language. Several of these transformations exploit the non-determinism present in IndiGolog programs. We also propose an automatic process to apply all those transformations on the basis of a set of parameters. Three case-studies show the significance of the approach in situations where agents are in interaction with human users.},
  eventtitle = {2013 {{IEEE}}/{{WIC}}/{{ACM International Joint Conferences}} on {{Web Intelligence}} ({{WI}}) and {{Intelligent Agent Technologies}} ({{IAT}})},
  isbn = {978-0-7695-5145-6},
  langid = {english},
  keywords = {\#nosource,conf,EIAH,VAE},
  file = {files/5557/Dubus et al. - 2013 - Parametric Reasoning Agents Extend the Control ove.pdf}
}

@inproceedings{dubusSituationCalculusPersonalized2011,
  title = {Situation Calculus and Personalized Web Systems},
  booktitle = {2011 11th {{International Conference}} on {{Intelligent Systems Design}} and {{Applications}}},
  author = {Dubus, Georges and Popineau, Fabrice and Bourda, Yolaine},
  date = {2011-11},
  pages = {569--574},
  publisher = {{IEEE}},
  location = {{Córdoba, Spain}},
  doi = {10/fzrtsq},
  abstract = {Personalized systems are a response to the increasing number of resources on the Internet, but can be difficult to create. In order to facilitate the design and creation of such personalized systems, we aim at formalizing them. The situation calculus is a logical framework that has often been proposed to model web applications and even personalized ones. However, the details of its use are much more rarely explained. In this paper we will show that it is needed to carefully consider which variant of the situation calculus to choose. We will precisely show why we want to use the so-called guarded action theories. We explain why and how it fits into an architecture. We introduce two scenarios of personalized applications to illustrate this choice.},
  eventtitle = {2011 11th {{International Conference}} on {{Intelligent Systems Design}} and {{Applications}}},
  keywords = {\#nosource,Adaptation models,adaptive systems,Adaptive systems,Calculus,Computer architecture,conf,EIAH,Intelligent systems,Noise measurement,personalization,Sensors,situation calculus,VAE,web application},
  file = {files/9100/Dubus et al. - 2011 - Situation calculus and personalized web systems.pdf}
}

@thesis{dubusTransformationProgrammesLogiques2014,
  type = {Thèse de doctorat},
  title = {Transformation de programmes logiques : application à la personnalisation et à la personnification d’agents.},
  shorttitle = {Transformation de programmes logiques},
  author = {Dubus, Georges},
  date = {2014-09-04},
  institution = {{Supélec}},
  url = {https://theses.fr/2014SUPL0017},
  urldate = {2021-08-07},
  abstract = {Cette thèse s'intéresse à la personnalisation et à la personnification d'agents intelligents dans le cadre d'applications web. Les techniques de personnalisation et de personnification sont de plus en plusutilisées pour mieux répondre aux besoins des utilisateurs. La plupart de ces techniques reposent sur des outils de raisonnement issus de l'intelligence artificielle. Cependant, ces techniques sont habituellement utilisés de manière ad-hoc pour chaque application. L'approche de cette thèse est de considérer la personnalisation et la personnification comme deux instances d'altération de comportement, et à étudier l'altération du comportements d'agents rationnels. Les principales contributions sont WAIG, un formalisme basé sur le langage Golog adapté à l'expression d'applications web, et PAGE, un cadre formel pour la manipulation et l'altération de programmes agents Golog, permettant la transformation automatique d'agent selon un critère donné. Ces contributions sont illustrés par plusieurs scénarios concrets issus des domaines de la personnalisation et de la personnification.},
  editora = {Bourda, Yolaine and Popineau, Fabrice},
  editoratype = {collaborator},
  langid = {french},
  keywords = {AI,IA,Intelligence artificielle,Logical programing,Personalization,Personification,Personnalisation,Personnification,Programmation logique,Thesis,VAE},
  file = {files/9165/Dubus - 2014 - Transformation de programmes logiques  applicatio.pdf}
}

@inproceedings{FRATANI_RM2021,
  title = {Ranking Geological Cross-Sections for Database Querying},
  booktitle = {2021 {{RING}} Meeting},
  author = {Fratani, Amandine and Viseur, Sophie and Popineau, Fabrice and Henry, Pierre and Ghattas, Badih and Oppenheim, Georges and Dhont, Damien and Gout, Claude},
  date = {2021},
  publisher = {{ASGA}},
  url = {https://www.ring-team.org/research-publications/ring-meeting-papers?view=pub&id=5051},
  abstract = {Geological cross-sections are convenient supports for synthesizing geological knowledge of given geological subsurface structures. They contain information about the geological time of the formations and their architecture. Querying a database of case study reports or papers with geological cross-sections could be then a convenient way to find geological analogues. Image-content query is used to achieve this kind of query. It is often performed using deep-learning, but these techniques require numerous pairs of original images and associated human-tagged images in order to obtain satisfying results after the training phase. Alternative approaches rely on image similarity measures based on textures, colours or shapes contained in the images. In this paper, we propose such a method for ranking geological cross-sections using two kinds of similarity measures: (i) the presence and proportion of formations from similar geological times; (ii) the global geological architecture. This approach combines the use of a colour dictionary and different correlation measures. Noddy is a software allowing synthetic geological cross-sections to be generated using a series of geological events (faulting, tilting, folding, etc.). A database of 100,000 geological cross-sections was generated using Noddy. In each cross-section, the layers are colored following the standard ICS color codes. Results of cross-section rankings are shown from this database and discussed.}
}

@article{groupedetravailtwg-tdsTDSStructureRepertoires2004,
  title = {TDS : une structure de répertoires pour les fichiers TeX},
  author = {Groupe de Travail TWG-TDS and Popineau, Fabrice},
  translator = {Charpentier, Jean-Côme},
  date = {2004},
  journaltitle = {Cahiers GUTenberg},
  number = {44–45},
  pages = {83--114},
  langid = {french},
  keywords = {\#nosource,⛔ No DOI found,journal,TeX},
  file = {files/9150/Groupe de Travail TWG-TDS et Popineau - 2004 - TDS  une structure de répertoires pour les fichie.pdf}
}

@inproceedings{hajriMORSSystemRecommending2017,
  title = {{{MORS}}: {{A System}} for {{Recommending OERs}} in a {{MOOC}}},
  shorttitle = {{{MORS}}},
  booktitle = {2017 {{IEEE}} 17th {{International Conference}} on {{Advanced Learning Technologies}} ({{ICALT}})},
  author = {Hajri, Hiba and Bourda, Yolaine and Popineau, Fabrice},
  date = {2017-07},
  pages = {50--52},
  doi = {10/ghfbvb},
  abstract = {Personalization in the field of Technology Enhanced Learning (℡) is a topic that received a lot of concern by researchers. At the same time, there is a growing amount of Open Educational Resources (OER) indexed according to the W3C standards. Relevant OERs can usefully complement the contents delivered to a learner during an online course. Computing the best OERs to offer to the learner at each point of his course is an aspect of personalization that we address in this paper. We designed our MORS system to solve this problem in the context of Massive Open Online Courses (MOOC). Our MORS system described in this paper, is based on a learner profile, on metadata describing the course and on a carefully crafted process to query the SparQL endpoints for OERs.},
  eventtitle = {2017 {{IEEE}} 17th {{International Conference}} on {{Advanced Learning Technologies}} ({{ICALT}})},
  keywords = {\#nosource,Computational modeling,Computer architecture,conf,Data mining,Education,EIAH,learner profile,learning,Metadata,MOOC,OER,Personalization,recommendation,Recommender systems,VAE,Videos},
  file = {files/9045/Hajri et al. - 2017 - MORS A System for Recommending OERs in a MOOC.pdf}
}

@incollection{hajriPersonalizedRecommendationOpen2018,
  title = {Personalized {{Recommendation}} of {{Open Educational Resources}} in {{MOOCs}}},
  booktitle = {Computer {{Supported Education}} - 10th {{International Conference}}, {{CSEDU}} 2018, {{Funchal}}, {{Madeira}}, {{Portugal}}, {{March}} 15-17, 2018, {{Revised Selected Papers}}},
  author = {Hajri, Hiba and Bourda, Yolaine and Popineau, Fabrice},
  editor = {McLaren, Bruce M. and Reilly, Rob and Zvacek, Susan and Uhomoibhi, James},
  date = {2018},
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {1022},
  pages = {166--190},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-21151-6_9},
  url = {http://link.springer.com/10.1007/978-3-030-21151-6_9},
  urldate = {2021-08-07},
  abstract = {Today Online Learning Environments (OLE) like MOOCs and LMS are very commonly used and a huge number of students with very different profiles and backgrounds follow the same online courses. Still, personalized experience for attendees is not widely spread on the platforms hosting these courses. At the same time, there is a growing number of open educational resources (OER) that can helpfully enrich the content of online courses and even be chosen to match one-by-one the student tastes. Recommender systems may support personalization in OLE by providing each learner with learning objects carefully selected to help reaching their learning objectives. This kind of recommendation is more specific to compute than usual recommendations like consumer products: the recommendation depends not only on the learner profile, but also on the content of the course, because the recommendation needs to fit precisely with the course format at any point. In this article, we introduce MOORS, a MOOC-based OER recommender system that can be plugged in an OLE to dynamically provide recommendations of OER to learners on the basis of their profiles and the profile of the MOOC. We also describe the process for calculating recommendations from OER metadata, assuming these metadata follow the Linked Opend Data (LOD) principles. Our implementation has been done in Open edX, an open source MOOC platform widely used, however the same approach could be implemented in any OLE as long as the learners profiles and the course profile can be extracted. Finally, we discuss a life-size evaluation of our recommender system.},
  isbn = {978-3-030-21150-9},
  langid = {english},
  keywords = {⚠️ Invalid DOI,conf,EIAH,inbook,VAE},
  file = {files/9036/Hajri et al. - 2018 - Personalized Recommendation of Open Educational Re.pdf}
}

@thesis{hajriPersonnalisationMOOCPar2018,
  type = {Thèse de doctorat},
  title = {Personnalisation des MOOC par la réutilisation de Ressources Éducatives Libres},
  author = {Hajri, Hiba},
  date = {2018-06-08},
  institution = {{Université Paris-Saclay (ComUE)}},
  url = {https://theses.fr/2018SACLC046},
  urldate = {2021-08-07},
  abstract = {La personnalisation de l’apprentissage dans les environnements informatiques pour l’apprentissage humain (EIAH) est un sujet de recherche qui est traité depuis de nombreuses années. Avec l’arrivée des cours en ligne ouverts et massifs (MOOC), la question de la personnalisation se pose de façon encore plus cruciale et de nouveaux défis se présentent aux chercheurs. En effet, le même MOOC peut être suivi par des milliers d’apprenants ayant des profils hétérogènes (connaissances, niveaux éducatif, objectifs, etc). Il devient donc nécessaire de tenir compte de cette hétérogénéité en présentant aux apprenants des contenus éducatifs adaptés à leurs profils afin qu’ils tirent parti au mieux du MOOC.D’un autre côté, de plus en plus de ressources éducatives libres (REL) sont partagées sur le web. Il est important de pouvoir réutiliser ces REL dans un contexte différent de celui pour lequel elles ont été créées. En effet, produire des REL de qualité est une activité coûteuse en temps et la rentabilisation des REL passe par leur réutilisation.Pour faciliter la découverte des REL, des schémas de métadonnées sont utilisés pour décrire les REL.Cependant, l’utilisation de ces schémas a amené à des entrepôts isolés de descriptions hétérogènes et qui ne sont pas interopérables. Afin de régler ce problème, une solution adoptée dans la littérature consiste à appliquer les principes des données ouvertes et liées (LOD) aux descriptions des REL.Dans le cadre de cette thèse, nous nous intéressons à la personnalisation des MOOC et à la réutilisation des REL.Nous proposons un système de recommandation qui fournit à un apprenant en train de suivre un MOOC des ressources externes qui sont des REL adaptées à son profil, tout en respectant les spécificités du MOOC suivi.Pour sélectionner les REL, nous nous intéressons à celles qui possèdent des descriptions insérées dans les LOD, stockées dans des entrepôts accessibles sur le web et offrant des moyens d’accès standardisés. Notre système de recommandation est implémenté dans une plateforme de MOOC, Open edX et il est évalué en utilisant une plateforme de micro-tâches.},
  editora = {Bourda, Yolaine and Popineau, Fabrice},
  editoratype = {collaborator},
  langid = {french},
  keywords = {Cours en ligne ouverts à tous,Données ouvertes,Données ouvertes et liées,Éducation et informatique,Linked open data,Mooc,Open educational resources,Personalization,Personnalisation,Ressources éducatives libres,Technologie éducative,Thesis,VAE},
  file = {files/9169/Hajri - 2018 - Personnalisation des MOOC par la réutilisation de .pdf}
}

@inproceedings{hajriQueryingRepositoriesOER2015,
  title = {Querying {{Repositories}} of {{OER Descriptions}}: {{The Challenge}} of {{Educational Metadata Schemas Diversity}}},
  shorttitle = {Querying {{Repositories}} of {{OER Descriptions}}},
  booktitle = {Design for {{Teaching}} and {{Learning}} in a {{Networked World}} - 10th {{European Conference}} on {{Technology Enhanced Learning}}, {{EC-TEL}} 2015},
  author = {Hajri, Hiba and Bourda, Yolaine and Popineau, Fabrice},
  date = {2015-09},
  series = {Design for {{Teaching}} and {{Learning}} in a {{Networked World}} - 10th {{European Conference}} on {{Technology Enhanced Learning}}, {{EC-TEL}} 2015},
  volume = {LNCS},
  pages = {582--586},
  publisher = {{Springer}},
  location = {{Toledo, Spain}},
  doi = {10/gmfpf6},
  url = {https://hal.archives-ouvertes.fr/hal-01275267},
  urldate = {2021-08-07},
  keywords = {\#nosource,⚠️ Invalid DOI,conf,EIAH,Linked Education,Linked Open Data,Metadata Schemas,VAE},
  file = {files/9059/Hajri et al. - 2015 - Querying Repositories of OER Descriptions The Cha.pdf}
}

@inproceedings{hajriSystemRecommendOpen2018,
  title = {A {{System}} to {{Recommend Open Educational Resources}} during an {{Online Course}}:},
  shorttitle = {A {{System}} to {{Recommend Open Educational Resources}} during an {{Online Course}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Computer Supported Education}}},
  author = {Hajri, Hiba and Bourda, Yolaine and Popineau, Fabrice},
  date = {2018-03},
  pages = {99--109},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  location = {{Funchal, Madeira, Portugal}},
  doi = {10/gmfn5w},
  url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006697000990109},
  urldate = {2021-08-07},
  eventtitle = {10th {{International Conference}} on {{Computer Supported Education}}},
  isbn = {978-989-758-291-2},
  langid = {english},
  keywords = {\#nosource,conf,EIAH,VAE},
  file = {files/9038/Hajri et al. - 2018 - A System to Recommend Open Educational Resources d.pdf}
}

@report{hawesPersonnalisationPersonnificationTuteurs2015,
  type = {Stage de fin d'études},
  title = {Personnalisation et Personnification des Tuteurs Intelligents},
  author = {Hawes, Hiba},
  date = {2015-09},
  institution = {{Université de la Manouba, École Nationale des Sciences de l'Informatique}},
  langid = {french},
  keywords = {\#nosource,Master,VAE}
}

@thesis{hayApprentissageRepresentationStyle2021,
  type = {Thèse de doctorat},
  title = {Apprentissage de la représentation du style écrit, application à la recommandation d'articles d'actualité},
  author = {Hay, Julien},
  date = {2021},
  institution = {{Université Paris-Saclay}},
  url = {https://tel.archives-ouvertes.fr/tel-03420487},
  abstract = {La modélisation des utilisateurs est une étape essentielle lorsqu'il s'agit de recommander des produits et proposer des services automatiquement. Les réseaux sociaux sont une ressource riche et abondante de données utilisateur (p. ex. liens partagés, messages postés) permettant de modéliser leurs intérêts et préférences. Dans cette thèse, nous proposons d'exploiter les articles d'actualité partagés sur les réseaux sociaux afin d'enrichir les modèles existants avec une nouvelle caractéristique textuelle : le style écrit. Cette thèse, à l'intersection des domaines du traitement automatique du langage naturel et des systèmes de recommandation, porte sur l'apprentissage de la représentation du style et de son application à la recommandation d'articles d'actualité. Dans un premier temps, nous proposons une nouvelle méthode d'apprentissage de la représentation du texte visant à projeter tout document dans un espace stylométrique de référence. L'hypothèse testée est qu'un tel espace peut être généralisé par un ensemble suffisamment large d'auteurs de référence, et que les projections vectorielles des écrits d'un auteur « nouveau » seront proches, d'un point de vue stylistique, des écrits d'un sous-ensemble consistant de ces auteurs de référence. Dans un second temps, nous proposons d'exploiter la représentation stylométrique du texte pour la recommandation d'articles d'actualité en la combinant à d'autres représentations (p. ex. thématique, lexicale, sémantique). Nous cherchons à identifier les caractéristiques les plus complémentaires pouvant permettre une recommandation d'articles plus pertinente et de meilleure qualité. L'hypothèse ayant motivé ces travaux est que les choix de lecture des individus sont non seulement influencés par le fond (p. ex. le thème des articles d'actualité, les entités mentionnées), mais aussi par la forme (c.-à-d. le style pouvant, par exemple, être descriptif, satirique, composé d'anecdotes personnelles, d'interviews). Les expérimentations effectuées montrent que non seulement le style écrit joue un rôle dans les préférences de lecture des individus, mais aussi que, lorsqu'il est combiné à d'autres caractéristiques textuelles, permet d'augmenter la précision et la qualité des recommandations en termes de diversité, de nouveauté et de sérendipité.},
  langid = {french},
  pagetotal = {260},
  keywords = {Thesis},
  file = {files/9951/Hay - 2021 - Apprentissage de la représentation du style écrit,.pdf}
}

@incollection{hayAutomaticallySelectingComplementary2018,
  title = {Automatically {{Selecting Complementary Vector Representations}} for {{Semantic Textual Similarity}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Management}}: {{Volume}} 8},
  author = {Hay, Julien and Van de Cruys, Tim and Muller, Philippe and Doan, Bich-Liên and Popineau, Fabrice and Hay, Julien and Van De Cruys, Tim and Muller, Philippe and Doan, Bich-Lien and Popineau, Fabrice and Ouassim, Ait-Elhara},
  editor = {Pinaud, Bruno and Guillet, Fabrice and Gandon, Fabien and Largeron, Christine},
  date = {2018},
  series = {Studies in {{Computational Intelligence}}},
  volume = {834},
  pages = {45--60},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  url = {https://doi.org/10.1007/978-3-030-18129-1_3},
  abstract = {The goal of the Semantic Textual Similarity task is to automatically quantify the semantic similarity of two text snippets. Since 2012, the task has been organized on a yearly basis as a part of the SemEval evaluation campaign. This paperHay, Julien~presentsVan de Cruys, Tim~a method that aims to combine differentMuller, Philippe~sentence-based vector representations in order to improve the computation of semantic similarity values. Our hypothesis is that such a combinationDoan, Bich-Liên~of different representations allows us to pinpoint different semantic aspects, which improves the accuracy ofPopineau, Fabrice~similarity computations. The method's main difficulty lies in the selection of the most complementaryAit-Elhara, Ouassim\textasciitilde{} representations, for which we present an optimization method. Our final system is based on the winning system of the 2015 evaluation campaign, augmented with the complementary vector representations selected by our optimization method. We also present evaluation results on the dataset of the 2016 campaign, which confirms the benefit of our method.},
  isbn = {978-3-030-18129-1},
  keywords = {⚠️ Invalid DOI,⛔ No DOI found,inbook,IR},
  file = {files/9030/Hay et al. - 2018 - Automatically selecting complementary vector repre.pdf}
}

@inproceedings{hayComplementaritesRepresentationsVectorielles2018,
  title = {Complémentarités de représentations vectorielles pour la similarité sémantique},
  booktitle = {18e Journees Francophones Extraction et Gestion de Connaissances (EGC 2018)},
  author = {Hay, Julien and Van De Cruys, Tim and Muller, Philippe and Doan, Biech Lien and Popineau, Fabrice and Benamsili, Lyes},
  date = {2018-01},
  volume = {E-34},
  pages = {179--190},
  publisher = {{Éditions RNTI}},
  location = {{Paris, France}},
  url = {https://hal.archives-ouvertes.fr/hal-02283159},
  urldate = {2021-08-07},
  abstract = {La tâche de similarité sémantique textuelle consiste à exprimer automatiquement un nombre reflétant la similarité sémantique de deux fragments de texte. Chaque année depuis 2012, les campagnes de SemEval déroulent cette tâche de similarité sémantique textuelle. Cet article présente une méthode associant différentes représentations vectorielles de phrases dans l'objectif d'améliorer les résultats obtenus en similarité sémantique. Notre hypothèse est que différentes représentations permettraient de représenter différents aspects sémantiques, et par extension, d'améliorer les similarités calculées, la principale difficulté étant de sélectionner les représentations les plus complémentaires pour cette tâche. Notre système se base sur le système vainqueur de la campagne de 2015 ainsi que sur notre méthode de sélection par complémentarité. Les résultats obtenus viennent confirmer l'intérêt de cette méthode lorsqu'ils sont comparés aux résultats de la campagne de 2016.},
  langid = {french},
  keywords = {⛔ No DOI found,confnat,IR,sémantique,Similarité},
  file = {files/9040/Hay et al. - 2018 - Complémentarités de représentations vectorielles p.pdf}
}

@inproceedings{hayFilteringReferenceCorpus2021,
  title = {Filtering a {{Reference Corpus}} to {{Generalize Stylometric Representations}}},
  author = {Hay, Julien and Doan, Bich-Liên and Popineau, Fabrice and Elhara, Ouassim},
  date = {2021-03-16},
  pages = {259--268},
  url = {https://www.scitepress.org/Link.aspx?doi=10.5220/0010138802590268},
  urldate = {2021-03-16},
  abstract = {Digital Library},
  eventtitle = {12th {{International Conference}} on {{Knowledge Discovery}} and {{Information Retrieval}}},
  isbn = {978-989-758-474-9},
  keywords = {\#nosource,conf,IR},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {files/9025/Hay et al. - 2021 - Filtering a Reference Corpus to Generalize Stylome.pdf}
}

@inproceedings{hayRepresentationLearningWriting2020,
  title = {Representation Learning of Writing Style},
  booktitle = {Proceedings of the {{Sixth Workshop}} on {{Noisy User-generated Text}} ({{W-NUT}} 2020)},
  author = {Hay, Julien and Doan, Bich-Lien and Popineau, Fabrice and Ait Elhara, Ouassim},
  date = {2020-11},
  pages = {232--243},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.wnut-1.30},
  url = {https://www.aclweb.org/anthology/2020.wnut-1.30},
  urldate = {2021-03-16},
  abstract = {In this paper, we introduce a new method of representation learning that aims to embed documents in a stylometric space. Previous studies in the field of authorship analysis focused on feature engineering techniques in order to represent document styles and to enhance model performance in specific tasks. Instead, we directly embed documents in a stylometric space by relying on a reference set of authors and the intra-author consistency property which is one of two components in our definition of writing style. The main intuition of this paper is that we can define a general stylometric space from a set of reference authors such that, in this space, the coordinates of different documents will be close when the documents are by the same author, and spread away when they are by different authors, even for documents by authors who are not in the set of reference authors. The method we propose allows for the clustering of documents based on stylistic clues reflecting the authorship of documents. For the empirical validation of the method, we train a deep neural network model to predict authors of a large reference dataset consisting of news and blog articles. Albeit the learning process is supervised, it does not require a dedicated labeling of the data but it relies only on the metadata of the articles which are available in huge amounts. We evaluate the model on multiple datasets, on both the authorship clustering and the authorship attribution tasks.},
  eventtitle = {{{EMNLP-WNUT}} 2020},
  keywords = {\#nosource,conf,IR},
  annotation = {ZSCC: 0000000},
  file = {files/8215/Hay et al. - 2020 - Representation learning of writing style.pdf}
}

@inproceedings{jacquiotGEAHSGenericEducational2004,
  title = {{{GEAHS}}: {{A Generic Educational Adaptive Hypermedia System}}},
  shorttitle = {{{GEAHS}}},
  author = {Jacquiot, Cedric and Bourda, Yolaine and Popineau, Fabrice},
  date = {2004},
  pages = {571--578},
  publisher = {{Association for the Advancement of Computing in Education (AACE)}},
  url = {https://www.learntechlib.org/primary/p/12989/},
  urldate = {2021-08-07},
  abstract = {GEAHS is a platform designed to ease the development of Adaptive Educational Hypermedia. In this document, we explain the underlying principles of this platform. Genericity is achieved thanks to an adaptation engine based on conceptual graphs, situation calculus and RDF. We are currently working on a graphical tool to build an AEH system designed on top of our engine. This paper describes the main aspects of our system, as well as the use we make of standards and recommendations.},
  eventtitle = {{{EdMedia}} + {{Innovate Learning}}},
  isbn = {978-1-880094-53-2},
  langid = {english},
  keywords = {conf,EIAH,VAE},
  file = {files/8978/Jacquiot et al. - 2004 - GEAHS A Generic Educational Adaptive Hypermedia S.pdf}
}

@inproceedings{jacquiotGEAHSGenericEducational2004a,
  title = {{{GEAHS}}: {{A Generic Educational Adaptive Hypermedia System Based}} on {{Situation Calculus}}},
  booktitle = {Adaptive Hypermedia and Adaptive Web-Based Systems, Third International Conference, {{AH}} 2004, Eindhoven, the Netherlands, August 23-26, 2004, Proceedings},
  author = {Jacquiot, Cédric and Bourda, Yolaine and Popineau, Fabrice},
  editor = {Bra, Paul De and Nejdl, Wolfgang},
  date = {2004},
  series = {Lecture Notes in Computer Science},
  volume = {3137},
  pages = {413--416},
  publisher = {{Springer}},
  doi = {10.1007/978-3-540-27780-4_63},
  url = {https://doi.org/10.1007/978-3-540-27780-4_63},
  abstract = {GEAHS is a platform designed to ease the development of Adaptive Educational Hypermedia, using standard formalisms. In this document, we explain the underlying principles of this platform. Genericity is achieved thanks to an adaptation engine based on situation calculus and RDF. This paper describes the main aspects of our system, as well as the use we make of situation calculus to create a simpler, more reusable adaptive hypermedia system.},
  isbn = {978-3-540-22895-0},
  langid = {english},
  keywords = {conf,EIAH,VAE},
  file = {files/9130/Jacquiot et al. - 2004 - GEAHS A Generic Educational Adaptive Hypermedia S.pdf}
}

@inproceedings{jacquiotGLAMGenericLayered2006,
  title = {{{GLAM}}: {{A}} Generic Layered Adaptation Model for Adaptive Hypermedia Systems},
  booktitle = {Adaptive Hypermedia and Adaptive Web-Based Systems, 4th International Conference, {{AH}} 2006, Dublin, Ireland, June 21-23, 2006, Proceedings},
  author = {Jacquiot, Cédric and Bourda, Yolaine and Popineau, Fabrice and Delteil, Alexandre and Reynaud, Chantal},
  editor = {V. Wade, H. Ashman and Smyth, B.},
  date = {2006-06},
  series = {{{LNCS}}},
  volume = {4018},
  pages = {131--140},
  publisher = {{Springer-Verlag}},
  keywords = {\#nosource,⚠️ Invalid DOI,conf,EIAH,VAE},
  file = {files/8975/Jacquiot et al. - 2006 - GLAM A generic layered adaptation model for adapt.pdf}
}

@thesis{jacquiotModelisationLogiqueGenerique2006,
  type = {Thèse de doctorat},
  title = {Modélisation logique et générique des systèmes d'hypermédias adaptatifs},
  author = {Jacquiot, Cédric},
  date = {2006-01-01},
  institution = {{Paris 11}},
  url = {https://theses.fr/2006PA112269},
  urldate = {2021-08-07},
  abstract = {Les hypermédias adaptatifs, comme tous les systèmes à base de connaissances, peuvent être divisés en deux parties : une partie statique, permettant la représentation de données relatives aux domaines à traiter, et une partie dynamique, consacrée au traitement des données par différents procédés. Les modèles de données existants sont souvent difficiles à réutiliser car ils sont soit très spécifiques à une application particulière, soit très généraux et, dans ce cas, il est rarement possible de les rendre plus spécifiques pour un domaine d’application particulier. Les modèles d’adaptation existants se limitent souvent à des langages de règles, qui ne font pas de distinction entre les différentes sortes de données. Cette thèse propose des modèles génériques de données, décrits sous forme de diagrammes de classe UML, permettant par spécialisation la création de modèles spécifiques à un domaine d’application. Elle présente également un modèle d’adaptation entièrement décrit en calcul des prédicats du premier ordre, basé sur la logique situationnelle, et muni d’un langage de règles constitué de plusieurs niveaux, prenant en compte les différents types de données à des niveaux différents du langage. Ce langage introduit, en outre, une forme de méta-adaptation, par sélection de stratégies de parcours du domaine. Cette thèse introduit la notion de parcours par tronçons, qui offre une solution intermédiaire, entre le parcours libre et le parcours guidé. L’ensemble des modèles est utilisé pour proposer une application dans le domaine du e-learning.},
  editora = {Reynaud, Chantal and Bourda, Yolaine and Popineau, Fabrice},
  editoratype = {collaborator},
  langid = {french},
  keywords = {E-learning,Méta-adaptation,Système de déduction,Thesis,VAE},
  annotation = {Prix de Thèse de la Fondation Jean-Luc Lagardère},
  file = {files/10005/Jacquiot - 2006 - Modélisation logique et générique des systèmes d'h.pdf}
}

@inproceedings{jacquiotReusabilityGEAHS2004,
  title = {Reusability in {{GEAHS}}},
  booktitle = {Engineering Advanced Web Applications: {{Proceedings}} of Workshops in Connection with the 4th International Conference on Web Engineering ({{ICWE}} 2004), Munich, Germany, 28-30 July, 2004},
  author = {Jacquiot, Cédric and Bourda, Yolaine and Popineau, Fabrice},
  editor = {Matera, Maristella and Comai, Sara},
  date = {2004},
  pages = {199--209},
  publisher = {{Rinton Press}},
  location = {{Munich, Germany}},
  url = {https://hal-supelec.archives-ouvertes.fr/hal-00263858},
  abstract = {GEAHS (Generic Educational Adaptive Hypermedia System) is a platform designed to ease the development of Adaptive Educational Hypermedia, using standard formalisms. In this document, we explain the underlying principles of this platform. Genericity is achieved thanks to an adaptation engine based on situation calculus and RDF. This paper describes the main aspects of our system, as well as the use we make of situation calculus to create a more reusable adaptive hypermedia system.},
  eventtitle = {International {{Workshop}} on {{Adaptive Hypermedia}} and {{Collaborative Web-based Systems}} ({{AHCW}} 2004) {{In}} Conjunction with {{International Conference}} on {{Web Engineering}} ({{ICWE}} 2004)},
  langid = {english},
  keywords = {\#nosource,⛔ No DOI found,conf,EIAH,VAE},
  file = {files/9120/Jacquiot et al. - 2004 - Reusability in GEAHS.pdf}
}

@inproceedings{jacquiotSEWAGOGenericAdaptive2003,
  title = {{{SEWAGO}} : {{A}} Generic Adaptive Educational Hypermedia System.},
  booktitle = {{{HYPERTEXT}} '03: {{Proceedings}} of the Fourteenth {{ACM}} Conference on {{Hypertext}} and {{Hypermedia}}},
  author = {Jacquiot, Cédric and Bourda, Yolaine and Popineau, Fabrice},
  date = {2003-08},
  location = {{Nottingham, UK}},
  abstract = {In this document, we explain the main principles and architecture of SEWAGO (SEveral WAys to GO), a generic adaptive educational hypermedia system we are working on. More precisely, SEWAGO is a platform for creating adaptive educational hypermedias, using the strictest and most reusable architecture. This system is based on first order logic and situation calculus (through a modified version of GoLog).},
  langid = {english},
  keywords = {\#nosource,⛔ No DOI found,EIAH,poster,VAE},
  file = {files/8977/Jacquiot et al. - 2003 - SEWAGO  A generic adaptive educational hypermedia.pdf}
}

@article{meguebliBetterNewsArticle2017,
  title = {Towards Better News Article Recommendation: {{With}} the Help of User Comments},
  shorttitle = {Towards Better News Article Recommendation},
  author = {Meguebli, Youssef and Kacimi, Mouna and Doan, Bich-Liên and Popineau, Fabrice},
  date = {2017-11},
  journaltitle = {World Wide Web},
  shortjournal = {World Wide Web},
  volume = {20},
  number = {6},
  pages = {1293--1312},
  issn = {1386-145X, 1573-1413},
  doi = {10.1007/s11280-017-0436-2},
  url = {http://link.springer.com/10.1007/s11280-017-0436-2},
  urldate = {2021-08-07},
  abstract = {News media platforms publish articles about daily events letting their users comment on them, and forming interesting discussions in almost real-time. To keep users always active and interested, media platforms need an effective recommender system to bring up new articles that match user interests. In this article, we show that we can improve the quality of recommendation by exploiting valuable information provided by user comments. This information reveals aspects not directly tackled by the news article on which they have been posted. We call such aspects latent aspects. We demonstrate how these latent aspects can make a crucial difference in the accuracy of future recommendation. The challenge in detecting them is due to the noisy nature of user comments. To support our claim, we propose a novel news recommendation system that (1) enriches the description of news articles by latent aspects extracted from user comments, (2) deals with noisy comments by proposing a model for user comments ranking, and (3) proposes a diversification model to remove redundancies and provide a wide coverage of aspects. We have tested our approach using large collections of real user activities in four news Web sites, namely The INDEPENDENT, The Telegraph, CNN and Al-Jazeera. The results show that our approach outperforms baseline approaches achieving a significantly higher accuracy.},
  langid = {english},
  keywords = {\#nosource,IR,journal},
  file = {files/9042/Meguebli et al. - 2017 - Towards better news article recommendation With t.pdf}
}

@inproceedings{meguebliBuildingRichUser2014,
  title = {Building Rich User Profiles for Personalized News Recommendation},
  booktitle = {Posters, Demos, Late-Breaking Results and Workshop Proceedings of the 22nd Conference on User Modeling, Adaptation, and Personalization Co-Located with the 22nd Conference on User Modeling, Adaptation, and Personalization ({{UMAP2014}})},
  author = {Meguebli, Youssef and Kacimi, Mouna and Doan, Bich-Liên and Popineau, Fabrice},
  date = {2014-07},
  pages = {33--40},
  location = {{Aalborg, Denmark}},
  keywords = {\#nosource,⛔ No DOI found,conf,IR,VAE},
  file = {files/8974/Meguebli et al. - 2014 - Building rich user profiles for personalized news .pdf}
}

@inproceedings{meguebliExploitingSocialDebates2014,
  title = {Exploiting {{Social Debates}} for {{Opinion Ranking}}},
  booktitle = {{{KDIR}} 2014 - Proceedings of the International Conference on Knowledge Discovery and Information Retrieval, Rome, Italy},
  author = {Meguebli, Youssef and Kacimi, Mouna and Doan, Bich-Liên and Popineau, Fabrice},
  date = {2014-10-21},
  pages = {250--260},
  publisher = {{SciTePress}},
  location = {{Rome, Italy}},
  doi = {10.5220/0005081702500260},
  abstract = {The number of opinions in news media platforms is increasing dramatically with daily news hits, and people spending more and more time to discuss topics and share experiences. Such user generated content represents a promising source for improving the effectiveness of news articles recommendation and retrieval. However, the corpus of opinions is often large and noisy making it hard to find prominent content. In this paper, we tackle this problem by proposing a novel scoring model that ranks opinions based on their relevance and prominence. We define the prominence of an opinion using its relationships with other opinions. To this end, we (1) create a directed graph of opinions where each link represents the sentiment an opinion expresses about another opinion (2) propose a new variation of the PageRank algorithm that boosts the scores of opinions along links with positive sentiments and decreases them along links with negative sentiments. We have tested the effectiveness of our model through extensive experiments using three datasets crawled from CNN, Independent, and The Telegraph Web sites . The experiments show that our scoring model achieves high quality results.},
  keywords = {\#nosource,conf,IR},
  file = {files/9080/Meguebli et al. - 2014 - Exploiting Social Debates for Opinion Ranking.pdf}
}

@inproceedings{meguebliHowHiddenAspects2014,
  title = {How {{Hidden Aspects Can Improve Recommendation}}?},
  booktitle = {6th {{International Conference}}, {{SocInfo}} 2014},
  author = {Meguebli, Youssef and Kacimi, Mouna and Doan, Bich-Liên and Popineau, Fabrice},
  date = {2014-11-11},
  series = {Social {{Informatics}}},
  volume = {8851},
  pages = {269--278},
  publisher = {{Springer}},
  location = {{Barcelone, Spain}},
  doi = {10/gmfpj3},
  url = {https://hal-supelec.archives-ouvertes.fr/hal-01105283},
  abstract = {Nowadays, more and more people are using online news platforms as their main source of information about daily life events. Users of such platforms discuss around topics providing new insights and sometimes revealing hidden aspects about topics. The valuable information provided by users needs to be exploited to improve the accuracy of news recommendation and thus keep users always motivated to provide comments. However, exploiting user generated content is very challenging due its noisy nature. In this paper, we address this problem by proposing a novel news recommendation system that (1) enrich the profile of news article with user generated content, (2) deal with noisy contents by proposing a ranking model for users’ comments, and (3) propose a diversification model for comments to remove redundancies and provide a wide coverage of topic aspects. The results show that our approach outperforms baseline approaches achieving high accuracy.},
  isbn = {978-3-319-13733-9},
  keywords = {\#nosource,⚠️ Invalid DOI,conf,IR},
  file = {files/9074/Meguebli et al. - 2014 - How Hidden Aspects Can Improve Recommendation.pdf}
}

@thesis{meguebliLeveragingUserGeneratedContent2015,
  type = {phdthesis},
  title = {Leveraging {{User-Generated Content}} for {{Enhancing}} and {{Personalizing News Recommendation}}.},
  author = {Meguebli, Youssef},
  date = {2015-03-27},
  institution = {{CentraleSupélec}},
  url = {https://tel.archives-ouvertes.fr/tel-01323014},
  urldate = {2021-08-07},
  abstract = {In this thesis, we have investigated how to exploit user-generated-content for personalized news recommendation purpose. The intuition behind this line of research is that the opinions provided by users, on news websites, represent a strong indicator about their profiles. We have addressed this problem by proposing three main contributions. Firstly, we have proposed a profile model that accurately describes both users’ interests and news article contents. The profile model was tested on three different applications ranging from identifying the political orientation of users to the context of news recommendation and the diversification of the list of recommended news articles. Results show that our profile model give much better results compared to state-of-the-art models. Secondly, we have investigated the problem of noise on opinions and how we can retrieve only relevant opinions in response to a given query.The proposed opinion ranking strategy is based on users’ debates features. We have used a variation of PageRank technique to define the score of each opinion. Results show that our approach outperforms two recent proposed opinions ranking strategies, particularly for controversial topics. Thirdly, we have investigated different ways of leveraging opinions on news article contents including all opinions, topk opinions based on opinion ranking strategy, and a set of diverse opinion. To extract a list of diverse opinions, we have employed a variation of an existing opinion diversification model. Results show that diverse opinions give the best performance over other leveraging strategies.},
  langid = {english},
  keywords = {Thesis},
  file = {files/9065/Meguebli - 2015 - Leveraging User-Generated Content for Enhancing an.pdf}
}

@inproceedings{meguebliNovelArchitectureSmart2012,
  title = {A {{Novel Architecture}} for a {{Smart Information Retrieval System Based}} on {{Opinions Engineering}}},
  booktitle = {Proceedings of the Sixth Symposium on Human-Computer Interaction and Information Retrieval {{HCIR}}'12},
  author = {Meguebli, Youssef and Doan, Bich-Liên and Popineau, Fabrice and Bourda, Yolaine},
  date = {2012-10-04},
  pages = {4 pages},
  location = {{Cambridge, United States}},
  url = {https://hal-supelec.archives-ouvertes.fr/hal-00765342},
  urldate = {2021-08-07},
  abstract = {In this paper, we present a novel architecture for personalized information retrieval (IR) and a simple scenario that illustrate the contribution of this architecture compared to current personalized IR. We use an extension of a Dung argumentation framework in order to improve the precision of our personalized information retrieval. We use also social media and search history to define the user-profile.},
  eventtitle = {{{ACM HCIR}} '12},
  langid = {english},
  keywords = {⛔ No DOI found,conf,IR},
  file = {files/9097/Meguebli et al. - 2012 - A Novel Architecture for a Smart Information Retri.pdf}
}

@inproceedings{meguebliPersonalizingInformationRetrieval2016,
  title = {Personalizing {{Information Retrieval Using}} an {{Extension}} of a {{Dung Argumentation}}},
  booktitle = {2016 {{IEEE Intl Conference}} on {{Computational Science}} and {{Engineering}} ({{CSE}}) and {{IEEE Intl Conference}} on {{Embedded}} and {{Ubiquitous Computing}} ({{EUC}}) and 15th {{Intl Symposium}} on {{Distributed Computing}} and {{Applications}} for {{Business Engineering}} ({{DCABES}})},
  author = {Meguebli, Youssef and Doan, Bich-Liên and Popineau, Fabrice},
  date = {2016-08},
  pages = {681--686},
  doi = {10/gmfpf5},
  url = {https://ieeexplore.ieee.org/document/7982323},
  abstract = {In this paper, we propose an architecture for personalizing information retrieval (IR), exploiting the interactions between the user and the social network. We use an extension of a Dung argumentation framework to show how the precision of the personalized information retrieval system could be improved. We use also social media and search history to define the user-profile which is represented by a restriction of a Description Logic.},
  eventtitle = {2016 {{IEEE Intl Conference}} on {{Computational Science}} and {{Engineering}} ({{CSE}}) and {{IEEE Intl Conference}} on {{Embedded}} and {{Ubiquitous Computing}} ({{EUC}}) and 15th {{Intl Symposium}} on {{Distributed Computing}} and {{Applications}} for {{Business Engineering}} ({{DCABES}})},
  keywords = {\#nosource,Blogs,conf,Conferences,Dung argumentation framework,EIAH,History,Indexes,Information retrieval,IR,opinions engineering,Query processing,social media,Social network services,User-profile model,VAE},
  file = {files/9056/Meguebli et al. - 2016 - Personalizing Information Retrieval Using an Exten.pdf}
}

@inproceedings{meguebliStoriesYouTwoStage2014,
  title = {Stories {{Around You A Two-Stage Personalized News Recommendation}}},
  booktitle = {{{KDIR}} 2014 - Proceedings of the International Conference on Knowledge Discovery and Information Retrieval},
  author = {Meguebli, Youssef and Kacimi, Mouna and Doan, Bich-Liên and Popineau, Fabrice},
  date = {2014-10-21},
  pages = {473--479},
  publisher = {{SciTePress}},
  location = {{Rome, Italy}},
  doi = {10/gg5vx3},
  abstract = {With the tremendous growth of published news articles, a key issue is how to help users find diverse and interesting news stories. To this end, it is crucial to understand and build accurate profiles for both users and news articles. In this paper, we define a user profile based on (1) the set of entities she/he talked about it in her/his comments and (2) the set of key-concepts related to those entities on which the user has expressed an opinion or a viewpoint. The same information is extracted from the content of each news article to create its profile. In a first step, we matched those profiles using a new similarity measure. We use also the news articles profiles to diversify the list of recommended stories in a second step. A first evaluation involving the activities of 150 real users in four news web sites, namely The Independent, The Telegraph, CNN and Aljazeera has shown the effectiveness of our approach compared to recent works.},
  keywords = {\#nosource,conf,IR},
  file = {files/9087/Meguebli et al. - 2014 - Stories Around You A Two-Stage Personalized News R.pdf}
}

@inproceedings{meguebliUnsupervisedApproachIdentifying2014,
  title = {Unsupervised Approach for Identifying Users' Political Orientations},
  booktitle = {Advances in Information Retrieval - 36th European Conference on {{IR}} Research, {{ECIR}} 2014, Amsterdam, the Netherlands, April 13-16, 2014. {{Proceedings}}},
  author = {Meguebli, Youssef and Kacimi, Mouna and Doan, Bich-Liên and Popineau, Fabrice},
  editor = {de Rijke, Maarten and Kenter, Tom and de Vries, Arjen P. and Zhai, ChengXiang and de Jong, Franciska and Radinsky, Kira and Hofmann, Katja},
  options = {useprefix=true},
  date = {2014},
  series = {Lecture Notes in Computer Science},
  volume = {8416},
  pages = {507--512},
  publisher = {{Springer}},
  doi = {10/ghfbvc},
  keywords = {\#nosource,⚠️ Invalid DOI,conf,IR},
  file = {files/9020/Meguebli et al. - 2014 - Unsupervised approach for identifying users' polit.pdf}
}

@book{norvigIntelligenceArtificielle4e2021,
  title = {Intelligence artificielle 4e édition},
  author = {Norvig, Peter and Russell, Stuart},
  editor = {Popineau, Fabrice},
  translator = {Popineau, Fabrice and Miclet, Laurent and {Claire Cadet}},
  date = {2021},
  edition = {4},
  publisher = {{Pearson}},
  location = {{Paris}},
  abstract = {La bible en intelligence artificielle. Cet ouvrage est LE manuel de référence en intelligence artificielle. C'est le seul ouvrage à couvrir de façon aussi complète et moderne tout le champ théorique et pratique de l’intelligence artificielle. C’est aussi le seul ouvrage à proposer une vision unifiée de l’intelligence artificielle, centrée autour de la notion d’agent intelligent. Les différents champs disciplinaires autour de l’IA sont abordés avec un très grand nombre de renvois entre les sections, ce qui constitue une richesse inestimable de l’ouvrage qui expose les connexions entre des domaines qui sont le plus souvent présentés comme indépendants. La 4e édition informe les lecteurs sur les dernières technologies, présente les concepts de manière plus unifiée et couvre de manière nouvelle ou élargie l’apprentissage automatique, l’apprentissage profond, l’apprentissage par transfert, les systèmes multi-agents, la robotique, le traitement du langage naturel, la causalité, la programmation probabiliste, le respect de la vie privée, l’équité et la sécurité.},
  isbn = {978-2-326-00221-0},
  langid = {french},
  keywords = {\#nosource,artificial intelligence,otherpub}
}

@article{popineauAffichezVosDocuments2000,
  title = {Affichez vos documents LaTeX sur le Web avec TeX4ht},
  author = {Popineau, Fabrice},
  date = {2000},
  journaltitle = {Cahiers GUTenberg},
  shortjournal = {Cah. GUT},
  number = {37-38},
  pages = {5--43},
  issn = {1140-9304},
  doi = {10/ghfbvh},
  url = {http://cahiers.gutenberg.eu.org/fitem?id=CG_2000___37-38_5_0},
  urldate = {2021-08-07},
  abstract = {Eitan Gurari is TEX4ht’s author, a clever tool which allows TEX and LATEX documents to be translated to html and xml. I’d like to show here that TEX4ht is of simple use, powerful and extensible. Let’s have a look at its features.},
  langid = {french},
  keywords = {journal,TeX},
  file = {files/9143/Popineau - 2000 - Affichez vos documents LaTeX sur le Web avec TeX4h.pdf}
}

@book{popineauAlgorithmesNotesCours2016,
  title = {Algorithmes : notes de cours},
  author = {Popineau, Fabrice},
  date = {2016},
  publisher = {{CentraleSupélec}},
  langid = {french},
  keywords = {\#nosource,poly}
}

@book{popineauANSICommonLisp2005,
  title = {ANSI Common Lisp},
  author = {Popineau, Fabrice},
  date = {2005},
  publisher = {{Supélec}},
  langid = {french},
  keywords = {\#nosource,LISP,poly}
}

@article{popineauDirectionsTEXLiveSystem2001,
  title = {Directions for the {{TEXLive}} System},
  author = {Popineau, Fabrice},
  date = {2001},
  journaltitle = {MAPS},
  number = {26},
  pages = {151--161},
  abstract = {This paper is about the current status of the TEXLive software. The first part of the paper will address the structured description of its content and how the Windows setup program can use it. The past experiments with the Windows installer have revealed that the problem was harder than expected. The new TEXLive 6 description files will allow a more effective way to use the setup program. Some further enhancements are even scheduled. The second part of the paper will address a set of possible extensions to the Web2C/Kpathsea pair (read it as a call for code contributions!). Some aspects of its use were not foreseen when it was devised and it may be time for an enhancement.},
  langid = {english},
  keywords = {⛔ No DOI found,conf,journal,TeX},
  file = {files/9139/Popineau - 2001 - Directions for the TEXLive system.pdf}
}

@book{popineauElementsInformatiqueTheorique2005,
  title = {Éléments d’Informatique Théorique},
  author = {Popineau, Fabrice},
  date = {2005},
  publisher = {{Supélec}},
  langid = {french},
  keywords = {\#nosource,poly}
}

@article{popineauFpTEXTeTEXbasedDistribution1999,
  title = {{{fpTEX}}: {{A teTEX-based Distribution}} for {{Windows}}},
  author = {Popineau, Fabrice},
  date = {1999},
  journaltitle = {TUGBoat},
  volume = {20},
  number = {3},
  pages = {290--297},
  abstract = {This paper deals with the ins and outs of porting the widely used teTEX distribution to the Windows environment. The choices made and difficulties experienced are related, a brief description of this huge distribution is given and the future work is sketched out.},
  langid = {english},
  keywords = {⛔ No DOI found,conf,TeX},
  file = {files/9145/Popineau - 1999 - fpTEX A teTEX-based Distribution for Windows.pdf}
}

@article{popineauFpTeXTeTeXPour1999,
  title = {fpTeX : teTeX pour Win32},
  shorttitle = {fpTeX},
  author = {Popineau, Fabrice},
  date = {1999},
  journaltitle = {Cahiers GUTenberg},
  shortjournal = {Cah. GUT},
  number = {32},
  pages = {47--61},
  issn = {1140-9304},
  doi = {10/ghfbvk},
  url = {http://cahiers.gutenberg.eu.org/fitem?id=CG_1999___32_47_0},
  urldate = {2021-08-07},
  langid = {french},
  keywords = {conf,TeX},
  file = {files/9147/Popineau - 1999 - fpTeX  teTeX pour Win32.pdf}
}

@inproceedings{popineauGeneratingAdaptiveHypermedia2003,
  title = {Generating Adaptive Hypermedia with {{Golog}} and Conceptual Graphs},
  booktitle = {Proceedings 3rd {{IEEE International Conference}} on {{Advanced Technologies}}},
  author = {Popineau, F. and Bourda, Y. and Doan, B.-L.},
  date = {2003-07},
  pages = {463--466},
  publisher = {{IEEE Computer Society}},
  location = {{Athens, Greece}},
  doi = {10/cq9bcv},
  abstract = {We present an adaptive hypermedia architecture based on Golog and conceptual graphs. We propose a conceptual graph model for the description of the hypermedia structure and the use of Golog, a logic programming language, for the generation of the personalized hypermedia. This approach has been validated by a prototype and two examples based on different domains.},
  eventtitle = {Proceedings 3rd {{IEEE International Conference}} on {{Advanced Technologies}}},
  keywords = {\#nosource,Calculus,conf,Education,EIAH,Engines,Logic programming,poster,Prototypes,Resource description framework,Robots,Semantic Web,Testing,VAE,Writing},
  file = {files/9136/Popineau et al. - 2003 - Generating adaptive hypermedia with Golog and conc.pdf}
}

@article{popineauInformacjaNtTeXLive2001,
  title = {Informacja Nt. {{TeXLive}} 6 (Information about {{TeXlive}} 6)},
  author = {Popineau, Fabrice and Wawrykiewicz, Stanislaw},
  date = {2001-04},
  keywords = {\#nosource,⛔ No DOI found,poster,TeX}
}

@book{popineauLogiqueRaisonnementArtificiel1993,
  title = {Logique du Raisonnement Artificiel},
  author = {Popineau, Fabrice},
  date = {1993},
  publisher = {{Supélec}},
  langid = {french},
  keywords = {\#nosource,poly}
}

@unpublished{popineauLuaTeXUnicode2007,
  title = {{{LuaTeX}} et {{Unicode}}},
  author = {Popineau, Fabrice},
  date = {2007-10-08},
  eventtitle = {Journée {{GUTenberg}} 2007},
  keywords = {conf,TeX},
  file = {files/9156/Popineau - 2007 - LuaTeX et Unicode.pdf}
}

@article{popineauMETAPOSTPratique2001,
  title = {METAPOST pratique},
  author = {Popineau, Fabrice},
  date = {2001},
  journaltitle = {Cahiers GUTenberg},
  shortjournal = {Cah. GUT},
  number = {41},
  pages = {167--175},
  issn = {1140-9304},
  doi = {10/ghfbvj},
  url = {http://cahiers.gutenberg.eu.org/fitem?id=CG_2001___41_167_0},
  urldate = {2021-08-07},
  abstract = {In this article, I will explain how to pratically use METAPOST. This program is very different from usual drawing programs, but it fits very well in a TEX based typesetting system.},
  langid = {french},
  keywords = {\#nosource,journal,TeX},
  file = {files/9141/Popineau - 2001 - METAPOST pratique.pdf}
}

@book{popineauModelisationOrienteeObjet1997,
  title = {Modélisation Orientée Objet},
  author = {Popineau, Fabrice},
  date = {1997},
  publisher = {{Supélec}},
  langid = {french},
  keywords = {\#nosource,poly}
}

@inproceedings{popineauPersonalizationPersonificationConstructive2014,
  title = {Personalization and Personification: {{A}} Constructive Approach Based on Parametric Agents},
  booktitle = {Intelligent Virtual Agents - 14th International Conference, {{IVA}} 2014},
  author = {Popineau, Fabrice and Dubus, Georges and Bourda, Yolaine},
  editor = {Bickmore, Timothy W. and Marsella, Stacy and Sidner, Candace L.},
  date = {2014-08},
  series = {Lecture Notes in Computer Science},
  volume = {8637},
  pages = {339--344},
  publisher = {{Springer}},
  location = {{Boston, MA, USA}},
  doi = {10.1007/978-3-319-09767-1\_44},
  url = {https://doi.org/10.1007/978-3-319-09767-1_44},
  keywords = {\#nosource,⚠️ Invalid DOI,conf,EIAH,VAE},
  file = {files/9089/Popineau et al. - 2014 - Personalization and personification A constructiv.pdf}
}

@article{popineauRapiditeSouplesseAvec1997,
  title = {Rapidité et Souplesse Avec Le Moteur {{Web2C-7}}},
  author = {Popineau, Fabrice},
  date = {1997},
  journaltitle = {Cahiers GUTenberg},
  number = {26},
  pages = {96--108},
  doi = {10/ghfbvg},
  keywords = {\#nosource,conf,TeX},
  file = {files/8965/Popineau - 1997 - Rapidité et souplesse avec le moteur Web2C-7.pdf}
}

@article{popineauStatusFutureTeXLive2002,
  title = {Status and Future of {{TeXLive}}},
  author = {Popineau, Fabrice},
  date = {2002-02},
  keywords = {\#nosource,⛔ No DOI found,poster,TeX}
}

@article{popineauTeXLiveWindowsWhat2002,
  title = {{{TeXLive}} under {{Windows}}: What's New with the 7ᵗʰ Edition?},
  author = {Popineau, Fabrice},
  date = {2002-09},
  journaltitle = {TUGBoat},
  volume = {23},
  number = {1},
  pages = {74--79},
  abstract = {The 7th edition of TEX Live under Windows has some new features that are explained in this paper. Especially notable are two experiments to extend Kpathsea beyond its original abilities: • sharing Kpathsea data-structures between several processes for faster processing, • allow url’s in client programs as well as filenames whenever they ask Kpathsea to open a file. Additional points about the TeXSetup.exe program and the evolution of other parts of the distribution will also be discussed.},
  langid = {english},
  keywords = {⛔ No DOI found,conf,TeX},
  file = {files/9137/Popineau - 2002 - TeXLive under Windows what's new with the 7ᵗʰ edi.pdf}
}

@unpublished{popineauUKTUGSpecialWin321999,
  title = {{{UK-TUG}} Special Win32 Meeting},
  author = {Popineau, Fabrice},
  date = {1999-05},
  keywords = {\#nosource,⛔ No DOI found,conf,TeX},
  file = {files/9152/Popineau - 1999 - UK-TUG special win32 meeting.pdf}
}

@article{popineauWindviUserManual1998,
  title = {Windvi User’s Manual},
  author = {Popineau, Fabrice},
  date = {1998},
  journaltitle = {MAPS},
  volume = {20},
  pages = {146--149},
  keywords = {\#nosource,⛔ No DOI found,journal,TeX},
  file = {files/8968/Popineau - 1998 - Windvi user’s manual.pdf}
}

@inproceedings{popineauWorkshopElicitingAdaptive2018,
  title = {Workshop Eliciting {{Adaptive Sequences}} for {{Learning}} ({{WeASeL}}) 2018},
  booktitle = {Proceedings of the 1st {{International Workshop}} Eliciting {{Adaptive Sequences}} for {{Learning}} Co-Located with the 14th {{International Conference}} on {{Intelligent Tutoring Systems}} ({{ITS}} 2018)},
  author = {Popineau, Fabrice and Valko, Michal and Vie, Jill-Jênn},
  editor = {Guin, Nathalie and Kumar, Amruth N.},
  date = {2018},
  series = {{{CEUR}} Workshop Proceedings},
  volume = {2354},
  pages = {141--143},
  publisher = {{CEUR-WS.org}},
  url = {http://ceur-ws.org/Vol-2354/w4preface.pdf},
  keywords = {\#nosource,⛔ No DOI found,conf,EIAH}
}

@unpublished{rahtzOccultSecretsTEXLive2003,
  title = {The {{Occult Secrets}} of {{TEXLive}}},
  author = {Rahtz, Sebastian and Popineau, Fabrice},
  date = {2003-08},
  eventtitle = {{{TUG}} 2003},
  langid = {english},
  venue = {{Hawaii}},
  keywords = {⛔ No DOI found,poster,TeX},
  file = {files/9154/Rahtz et Popineau - 2003 - The Occult Secrets of TEXLive.pdf}
}

@book{russelIntelligenceArtificielle3e2010,
  title = {Intelligence artificielle 3e édition : Avec plus de 500 exercices},
  shorttitle = {Intelligence artificielle 3e édition},
  author = {Russel, Stuart and Norvig, Peter},
  translator = {Popineau, Fabrice},
  date = {2010-12-10},
  publisher = {{PEARSON}},
  location = {{Paris}},
  abstract = {Écrit par les experts de renommée mondiale, ce livre est la référence incontournable en matière d'intelligence artificielle (IA) dont il présente et analyse tous les concepts : logique, probabilités, mathématiques discrètes et du continu, perception, raisonnement, apprentissage, prise de décision et action. Sa spécificité est de présenter l'IA à travers le concept des agents intelligents. Les auteurs exposent comment un système réussit à percevoir son environnement de manière à analyser ce qu'il s'y passe, et comment il transforme la perception qu'il a de son environnement en actions concrètes. Parmi les sujets couverts : - les contributions historiques des mathématiques, de la théorie des jeux, de l'économie, de la théorie des probabilités, de la psychologie, de la linguistique et des neurosciences; - les méthodes qui permettent de prendre des décisions lors de l'établissement d'un projet, en tenant compte des étapes à venir; - les différentes manières de représenter formellement les connaissances relatives au monde qui nous entoure ainsi que le raisonnement logique fondé sur ces connaissances; - les méthodes de raisonnement qui permettent d'établir des plans et donc de proposer des actions à entreprendre; - la prise de décisions en environnement incertain : réseaux bayésiens et algorithmes tels que l'élimination de variables et MCMC (Markov Chain Monte-Carlo); - les méthodes employées pour générer les connaissances exigées par les composants de prise de décision : les algorithmes de boosting, l'algorithme EM (expectation-minimization), l'apprentissage à base d'exemples et les méthodes à noyaux (machines à vecteurs support); - les implications philosophiques et éthiques de l'IA. Chaque chapitre est illustré par de nombreux exemples et s'achève par des activités, qui vont des exercices de réflexion à des exercices de programmation, en passant par l'approfondissement des méthodes décrites, soit plus de 500 activités au total. Cette 3e édition tient compte des derniers développements de la matière, concernant notamment les représentations qu'un agent peut utiliser (atomique, factorisée, structurée), les environnements partiellement observables et non déterministes, les planifications contingente et hiérarchique, les modèles probabilistes du premier ordre, l'apprentissage automatique, la recherche et l'extraction d'information sur le web et l'apprentissage à partir de très grandes bases de données.},
  isbn = {978-2-7440-7455-4},
  langid = {french},
  pagetotal = {1200},
  keywords = {\#nosource,artificial intelligence,otherpub}
}

@book{russellIntelligenceArtificielle2nde2006,
  title = {Intelligence artificielle 2nde édition},
  author = {Russell, Stuart and Norvig, Peter},
  translator = {Miclet, Laurent and Popineau, Fabrice and Baland, Marie-Cécile},
  date = {2006-09-13},
  publisher = {{PEARSON}},
  location = {{Paris}},
  abstract = {Il aborde:- Tous les aspects de la discipline : logique, probabilits et mathmatiques du continu, perception, raison-nement, apprentissage et action. - Les diffrentes mthodes utiles la prise de dcision lors de la ralisation de projets. - Lapplication de ces mthodes en fonction des diffrentes possibilits de reprsentation, notamment lors de la construction de plans. - Les diffrents domaines dapplication : langage crit et parl, perception visuelle, robotiqueLintrt de ce manuel est de prsenter lIA travers le concept dagents intelligents (systmes qui dcident de ce qu'il convient de faire). Il dcrit: - La perception de lenvironnement par lagent intelligent pour dterminer et analyser ce quil s'y passe (par la vision, le toucher, l'oue ou le langage). - La transformation de la perception en actions concrtes. Dans cette optique, la robotique et la vision sont traites dans le cadre de leur contribution l'obtention de buts, et les auteurs insistent sur l'importance de l'environnement. Ainsi, certains agents doivent rsoudre des problmes combinatoires (jeux, labyrinthes, satisfaction de contraintes), dautres raisonnent (logique, dmonstration), dautres encore apprennent. Prs de 400 exercices sont proposs. Ils vont des exercices de rflexion aux exercices de programmation, en passant par lapprofondissement des mthodes dcrites.},
  isbn = {978-2-7440-7150-8},
  langid = {french},
  pagetotal = {1216},
  keywords = {\#nosource,artificial intelligence,otherpub}
}

@book{schaaEuroTeX20052005,
  title = {{{EuroTeX}} 2005},
  author = {Schaa, Volker RW and Popineau, Fabrice and André, Jacques and Hagen, Hans and Flipo, Daniel and Jackowski, Bogusław and Bzyl, Włodzimierz and Sojka, Petr and Bilotta, Giuseppe and Grathwohl, Steve and Raichle, Bernd},
  date = {2005-03},
  series = {{{TUGBoat}}},
  edition = {TeX Users Group},
  volume = {85},
  location = {{Abbaye des Prémontrés, Pont-à-Mousson, France}},
  langid = {english},
  pagetotal = {164},
  keywords = {⛔ No DOI found,conf,TeX},
  file = {files/9113/Schaa et al. - 2005 - EuroTeX 2005.pdf}
}

@inproceedings{vieAdaptiveTestingUsing2016,
  title = {Adaptive {{Testing Using}} a {{General Diagnostic Model}}},
  booktitle = {11th {{European Conference}} on {{Technology Enhanced Learning}}, {{EC-}}℡ 2016},
  author = {Vie, Jill-Jênn and Popineau, Fabrice and Bruillard, Eric and Bourda, Yolaine},
  date = {2016-09},
  series = {Lecture Notes in Computer Science},
  volume = {9891},
  pages = {331--339},
  publisher = {{Springer International Publishing}},
  location = {{Lyon, France}},
  url = {https://hal.inria.fr/hal-01376944},
  urldate = {2021-08-07},
  abstract = {In online learning platforms such as MOOCs, computerized assessment needs to be optimized in order to prevent boredom and dropout of learners. Indeed, they should spend as little time as possible in tests and still receive valuable feedback. It is actually possible to reduce the number of questions for the same accuracy with computerized adaptive testing (CAT): asking the next question according to the past performance of the examinee. CAT algorithms are divided in two categories: summative CATs, that measure the level of examinees, and formative CATs, that provide feedback to the examinees at the end of the test by specifying which knowledge components need further work. In this paper, we formalize the problem of test-size reduction by predicting student performance, and propose a new hybrid CAT algorithm GenMA based on the general diagnostic model, that is both summative and formative. Using real datasets, we compare our model to popular CAT models and show that GenMA achieves better accuracy while using fewer questions than the existing models.},
  keywords = {⚠️ Invalid DOI,conf,EIAH},
  file = {files/9053/Vie et al. - 2016 - Adaptive Testing Using a General Diagnostic Model.pdf}
}

@article{vieAutomatedTestAssembly2018,
  title = {Automated {{Test Assembly}} for {{Handling Learner Cold-Start}} in {{Large-Scale Assessments}}},
  author = {Vie, Jill-Jênn and Popineau, Fabrice and Bruillard, Eric and Bourda, Yolaine},
  date = {2018},
  journaltitle = {Int. J. Artif. Intell. Educ.},
  volume = {28},
  number = {4},
  pages = {616--631},
  doi = {10/gdvnnj},
  url = {https://doi.org/10.1007/s40593-017-0163-y},
  keywords = {⛔ No DOI found,EIAH,journal},
  file = {files/8973/Vie et al. - 2018 - Automated Test Assembly for Handling Learner Cold-.pdf}
}

@inproceedings{vieHeuristicMethodLargeScale2017,
  title = {A {{Heuristic Method}} for {{Large-Scale Cognitive-Diagnostic Computerized Adaptive Testing}}},
  booktitle = {Proceedings of the {{Fourth}} (2017) {{ACM Conference}} on {{Learning}} @ {{Scale}}},
  author = {Vie, Jill-Jênn and Popineau, Fabrice and Tort, Françoise and Marteau, Benjamin and Denos, Nathalie},
  date = {2017-04-12},
  pages = {323--326},
  publisher = {{ACM}},
  location = {{Cambridge Massachusetts USA}},
  doi = {10/ghfbvf},
  url = {https://dl.acm.org/doi/10.1145/3051457.3054015},
  urldate = {2021-08-07},
  abstract = {In formative assessments, one wants to provide a useful feedback to the examinee at the end of the test. In order to reduce the number of questions asked in an assessment, adaptive testing models have been developed for cognitive diagnosis, such as the ones encountered in knowledge space theory. However, when the number of skills assessed is very huge, such methods cannot scale. In this paper, we present a new method to provide adaptive tests and useful feedback to the examinee, even with large databases of skills. It will be used in Pix, a platform for certification of digital competencies for every French citizen.},
  eventtitle = {L@{{S}} 2017: {{Fourth}} (2017) {{ACM Conference}} on {{Learning}} @ {{Scale}}},
  isbn = {978-1-4503-4450-0},
  langid = {english},
  keywords = {EIAH,poster},
  file = {files/9047/Vie et al. - 2017 - A Heuristic Method for Large-Scale Cognitive-Diagn.pdf}
}

@thesis{vieModelesTestsAdaptatifs2016,
  type = {Thèse de doctorat},
  title = {Modèles de tests adaptatifs pour le diagnostic de connaissances dans un cadre d'apprentissage à grande échelle},
  author = {Vie, Jill-Jênn},
  date = {2016-12-05},
  institution = {{Université Paris-Saclay (ComUE)}},
  url = {https://theses.fr/2016SACLC090},
  urldate = {2021-08-07},
  abstract = {Cette thèse porte sur les tests adaptatifs dans les environnements d’apprentissage. Elle s’inscrit dans les contextes de fouille de données éducatives et d’analytique de l’apprentissage, où l’on s’intéresse à utiliser les données laissées par les apprenants dans des environnements éducatifs pour optimiser l’apprentissage au sens large.L’évaluation par ordinateur permet de stocker les réponses des apprenants facilement, afin de les analyser et d’améliorer les évaluations futures. Dans cette thèse, nous nous intéressons à un certain type de test par ordinateur, les tests adaptatifs. Ceux-ci permettent de poser une question à un apprenant, de traiter sa réponse à la volée, et de choisir la question suivante à lui poser en fonction de ses réponses précédentes. Ce processus réduit le nombre de questions à poser à un apprenant tout en conservant une mesure précise de son niveau. Les tests adaptatifs sont aujourd’hui implémentés pour des tests standardisés tels que le GMAT ou le GRE, administrés à des centaines de milliers d’étudiants. Toutefois, les modèles de tests adaptatifs traditionnels se contentent de noter les apprenants, ce qui est utile pour l’institution qui évalue, mais pas pour leur apprentissage. C’est pourquoi des modèles plus formatifs ont été proposés, permettant de faire un retour plus riche à l’apprenant à l’issue du test pour qu’il puisse comprendre ses lacunes et y remédier. On parle alors de diagnostic adaptatif.Dans cette thèse, nous avons répertorié des modèles de tests adaptatifs issus de différents pans de la littérature. Nous les avons comparés de façon qualitative et quantitative. Nous avons ainsi proposé un protocole expérimental, que nous avons implémenté pour comparer les principaux modèles de tests adaptatifs sur plusieurs jeux de données réelles. Cela nous a amenés à proposer un modèle hybride de diagnostic de connaissances adaptatif, meilleur que les modèles de tests formatifs existants sur tous les jeux de données testés. Enfin, nous avons élaboré une stratégie pour poser plusieursquestions au tout début du test afin de réaliser une meilleure première estimation des connaissances de l’apprenant. Ce système peut être appliqué à la génération automatique de feuilles d’exercices, par exemple sur un cours en ligne ouvert et massif (MOOC).},
  editora = {Bourda, Yolaine and Bruillard, Éric and Popineau, Fabrice},
  editoratype = {collaborator},
  langid = {french},
  keywords = {Adaptive testing,Analytique de l'apprentissage,Cognitive diagnosis,Cours en ligne ouverts à tous,Cours en ligne ouverts et massifs (MOOC),Diagnostic de connaissances,Exploration de données,Formation en ligne,Item response theory,Learning analytics,Massive open online courses (MOOCs),Q-Matrice,Q-Matrix,Systèmes adaptatifs (informatique),Technologie éducative,Tests adaptatifs,Théorie de la réponse à l'item,Thesis},
  file = {files/9173/Vie - 2016 - Modèles de tests adaptatifs pour le diagnostic de .pdf}
}

@inproceedings{viePredictingPerformanceDichotomous2015,
  title = {Predicting {{Performance}} on {{Dichotomous Questions}}: {{Comparing Models}} for {{Large-Scale Adaptive Testing}}},
  shorttitle = {Predicting {{Performance}} over {{Dichotomous Questions}}},
  booktitle = {Proceedings of the 8th International Conference on Educational Data Mining, {{EDM}} 2015, Madrid, Spain, June 26-29, 2015},
  author = {Vie, Jill-Jênn and Popineau, Fabrice and Grill, Jean-Bastien and Bruillard, Eric and Bourda, Yolaine},
  date = {2015-06},
  pages = {618--619},
  publisher = {{International Educational Data Mining Society (IEDMS)}},
  url = {http://www.educationaldatamining.org/EDM2015/proceedings/poster618-619.pdf},
  urldate = {2021-08-07},
  abstract = {Computerized adaptive testing (CAT) is a mode of testing which has gained increasing popularity over the past years. It selects the next question to ask to the examinee in order to evaluate her level efficiently, by using her answers to the previous questions. Traditionally, CAT systems have been relying on item response theory (IRT) in order to provide an effective measure of latent abilities in possibly large-scale assessments. More recently, from the perspective of providing useful feedback to examinees, other models have been studied for cognitive diagnosis. One of them is the q-matrix model, which draws a link between questions and examinee knowledge components. In this paper, we define a protocol based on performance prediction to evaluate adaptive testing algorithms. We use it to evaluate q-matrices in the context of assessments and compare their behavior to item response theory. Results computed on three real datasets of growing size and of various nature suggest that tests of different type need different models.},
  langid = {english},
  keywords = {\#nosource,⛔ No DOI found,conf,EIAH,poster},
  file = {files/9071/Vie et al. - 2015 - Predicting Performance on Dichotomous Questions C.pdf}
}

@inproceedings{viePredictionPerformanceQuestions2015,
  title = {Prédiction de performance sur des questions dichotomiques: comparaison de modèles pour des tests adaptatifs à grande échelle},
  shorttitle = {Prédiction de performance sur des questions dichotomiques},
  booktitle = {Atelier Évaluation des Apprentissages et Environnements Informatiques, EIAH 2015},
  author = {Vie, Jill-Jênn and Popineau, Fabrice and Grill, Jean-Bastien and Bruillard, Eric and Bourda, Yolaine},
  date = {2015-06},
  publisher = {{International Educational Data Mining Society (IEDMS)}},
  location = {{Agadir, Morocco}},
  url = {https://hal.archives-ouvertes.fr/hal-01275277},
  urldate = {2021-08-07},
  abstract = {Les tests adaptatifs sont un moyen d'évaluation qui a récemment gagné en popularité. Ils sélectionnent la prochaine question à poser à un examiné de manière à estimer son niveau efficacement, en fonction de ses réponses aux questions précédentes. Les systèmes de tests adaptatifs se sont d'abord appuyés sur la théorie de la réponse à l'item (TRI) afin de fournir une mesure efficace des traits latents dans des évaluations pouvant être à grande échelle. Plus récemment, dans l'optique de fournir un retour utile à l'examiné, d'autres modèles ont été étudiés dans la théorie du diagnostic cognitif. L'un d'eux est le modèle qmatrice, qui établit un lien entre questions du test et compétences de l'examiné. Dans cet article, nous définissons un protocole basé sur la prédiction de performance pour évaluer deux modèles de tests adaptatifs : qmatrice et le modèle de Rasch issu de la TRI. Les résultats obtenus sur trois jeux de données réelles de différente taille et nature montrent que selon les caractéristiques du test, l'un ou l'autre modèle prédit le mieux la performance de l'examiné.},
  langid = {french},
  keywords = {\#nosource,⛔ No DOI found,conf,EIAH,poster},
  file = {files/9062/Vie et al. - 2015 - Prédiction de performance sur des questions dichot.pdf}
}

@incollection{vieReviewRecentAdvances2017,
  title = {A {{Review}} of {{Recent Advances}} in {{Adaptive Assessment}}},
  booktitle = {Learning {{Analytics}}: {{Fundaments}}, {{Applications}}, and {{Trends}}},
  author = {Vie, Jill-Jênn and Popineau, Fabrice and Bruillard, Éric and Bourda, Yolaine},
  date = {2017-02},
  series = {Studies in {{Systems}}, {{Decision}} and {{Control}}},
  volume = {94},
  pages = {113--142},
  publisher = {{Springer International Publishing}},
  url = {https://hal.archives-ouvertes.fr/hal-01488284},
  urldate = {2021-08-07},
  abstract = {Computerized assessments are an increasingly popular way to evaluate students. They need to be optimized so that students can receive an accurate evaluation in as little time as possible. Such optimization is possible through learning analytics and computerized adaptive tests (CATs): the next question is then chosen according to the previous responses of the student, thereby making assessment more efficient. Using the data collected from previous students in non-adaptive tests, it is thus possible to provide formative adaptive tests to new students by telling them what to do next. This chapter reviews several models of CATs found in various fields, together with their main characteristics. We then compare these models empirically on real data. We conclude with a discussion of future research directions for computerized assessments.},
  keywords = {\#nosource,adaptive testing,cognitive diagnosis models,EIAH,inbook,item response theory,knowledge space theory,latent knowledge extraction,q-matrix},
  file = {files/9050/Vie et al. - 2017 - A Review of Recent Advances in Adaptive Assessment.pdf}
}

@article{vieSimulationValidationTests2017,
  title = {Simulation et Validation de Tests Adaptatifs Dans Les {{MOOC}}},
  author = {Vie, Jill-Jênn and Popineau, Fabrice and Bruillard, Eric and Bourda, Yolaine},
  date = {2017},
  journaltitle = {Sciences et Technologies de l'Information et de la Communication pour l'Éducation et la Formation},
  volume = {24},
  number = {2},
  pages = {149--167},
  doi = {10/ghfbvd},
  keywords = {\#nosource,EIAH,journalnat},
  file = {files/9019/Vie et al. - 2017 - Simulation et validation de tests adaptatifs dans .pdf}
}

@inproceedings{zemirlineAssistingReuseAdaptive2008,
  title = {Assisting in {{Reuse}} of {{Adaptive Hypermedia Creator}}’s {{Models}}},
  booktitle = {Adaptive {{Hypermedia}} and {{Adaptive Web-Based Systems}}},
  author = {Zemirline, Nadjet and Bourda, Yolaine and Reynaud, Chantal and Popineau, Fabrice},
  editor = {Nejdl, Wolfgang and Kay, Judy and Pu, Pearl and Herder, Eelco},
  date = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {5149},
  pages = {357--360},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Hannover,, Germany}},
  doi = {10.1007/978-3-540-70987-9\_53},
  url = {http://link.springer.com/10.1007/978-3-540-70987-9_53},
  urldate = {2019-05-08},
  abstract = {The design of Adaptive Hypermedia is a difficult task which can be made easier if generic systems and AH creators’ models are reused. We address this design problem in the setting of the GLAM platform only made up of generic components. We present a rule-based approach helping an AH creator in reusing its user and domain models to create a specific adaptive hypermedia. This semi-automatic approach takes the creator’s models as specialisations of GLAM generic models and requires the creator to express a minimum set of mappings between his models and the generic ones. The process results in a merged model consisting of the generic and the corresponding specific model. This merged model can be used by the adaptation model.},
  eventtitle = {5th International Conference, {{AH}} 2008},
  isbn = {978-3-540-70984-8},
  langid = {english},
  keywords = {\#nosource,⚠️ Invalid DOI,conf,EIAH,VAE}
}

@inproceedings{zemirlineMESAMProtegePlugin2009,
  title = {{{MESAM}}: {{A Protégé Plug-in}} for the {{Specialization}} of {{Models}}},
  shorttitle = {{{MESAM}}},
  booktitle = {11th {{International Protégé Conference}}},
  author = {Zemirline, Nadjet and Bourda, Yolaine and Reynaud, Chantal and Popineau, Fabrice},
  date = {2009-06},
  pages = {3 pages},
  location = {{Amsterdam, Netherlands}},
  url = {https://hal-supelec.archives-ouvertes.fr/hal-00431319},
  urldate = {2021-08-07},
  abstract = {Nowadays, several efforts are focused on re-using generic platforms to create new systems, in order to make the design process easier and faster. Often, the designer has his own models and resources and would like to reuse the generic system over his resources. That means, he has to integrate his models and resources in the system, and then to directly reuse the generic system. But many problems occur. One of them is that the designer needs to translate his models into the specific format that understood by the system and to use the vocabulary specific to that system. Furthermore, he also needs to translate all the instantiations of his models (i.e. the resources and their metadata). We think that this task is tedious and time-consuming and we want to avoid it. Our objective is to allow the designer to reuse his models (his vocabulary) and his models' instantiations without any change of format or vocabulary. For example, a generic Adaptive Hypermedia System (AHS) is made of a generic adaptation model relying on generic user and domain models. The designer would like to integrate his models and instances in the generic models in order to reuse the generic adaptation engine. Specific systems can be obtained by specializing the generic models. However, this specialization process is not always easy to perform. It has to be supported to make the design process easier and faster. This paper focuses on assisting designers to specialize generic models using their own models. We aim to automate this process which has been so far entirely manual. Our objectives are twofold: to create a support for defining mappings between elements in generic models and elements in the designer's personal models and to help creating consistent and relevant models integrating the generic and specific ones and taking into account the mappings between them. The proposed approach relies on OWL1, a W3C standard and SWRL2, a W3C proposal.},
  keywords = {conf,EIAH,VAE},
  file = {files/9107/Zemirline et al. - 2009 - MESAM A Protégé Plug-in for the Specialization of.pdf}
}

@inproceedings{zemirlinePatternRulebasedApproach2008,
  title = {A Pattern and Rule-Based Approach for Reusing Adaptive Hypermedia Creator's Models},
  booktitle = {Proceedings of 16th International Conference on Knowledge Engineering and Knowledge Management Knowledge Patterns},
  author = {Zemirline, Nadjet and Reynaud, Chantal and Bourda, Yolaine and Popineau, Fabrice},
  editor = {Gangemi, Aldo and Euzenat, Jérôme},
  date = {2008},
  series = {Lecture Notes in Computer Science},
  volume = {5268},
  pages = {17--31},
  publisher = {{Springer-Verlag}},
  location = {{Acitrezza, Catania, Italie}},
  doi = {10.1007/978-3-540-87696-0\_5},
  url = {https://doi.org/10.1007/978-3-540-87696-0_5},
  keywords = {⚠️ Invalid DOI,❓ Multiple DOI,conf,EIAH,VAE},
  file = {files/9109/Zemirline et al. - 2008 - A pattern and rule-based approach for reusing adap.pdf}
}


